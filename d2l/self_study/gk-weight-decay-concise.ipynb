{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet.gluon.loss import L2Loss\n",
    "import mxnet.gluon.nn as nn\n",
    "from mxnet.gluon.trainer import Trainer\n",
    "from mxnet import init\n",
    "from mxnet import autograd\n",
    "from pdb import set_trace\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 200\n",
    "n = 200\n",
    "X = np.random.normal(size=(n, p))\n",
    "w = np.random.normal(size=(p, 1))\n",
    "b = np.random.normal()\n",
    "y = np.dot(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(X)\n",
    "train_size_fraction = 0.1\n",
    "shuffled_indices = np.random.shuffle(np.array(range(dataset_size)))\n",
    "train_indices = shuffled_indices[:int(train_size_fraction*dataset_size)]\n",
    "test_indices = shuffled_indices[int(train_size_fraction*dataset_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = (X[train_indices, :], y[train_indices]), (X[test_indices, :], y[test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iterator(ds, batch_size=100):\n",
    "    ds_size = len(ds[0])\n",
    "    for start_idx in range(0, ds_size, batch_size):\n",
    "        indices = range(start_idx, min(start_idx + batch_size, ds_size))\n",
    "        yield ds[0][indices], ds[1][indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = get_iterator(train_ds), get_iterator(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "with model.name_scope():\n",
    "    model.add(nn.Dense(1))\n",
    "model.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(yhat, y):\n",
    "    return L2Loss()(yhat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_loss(ds, model):\n",
    "    mse = 0\n",
    "    num_samples = 0\n",
    "    for X, y in get_iterator(ds):\n",
    "        yhat = model(X)\n",
    "        mse += ((yhat - y)**2).sum()\n",
    "        num_samples += y.shape[0]\n",
    "    return mse/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size=20\n",
      "Epoch: 0, train_loss: 11.905947, test_loss: 214.86436\n",
      "Epoch: 1, train_loss: 11.9028, test_loss: 214.86375\n",
      "Epoch: 2, train_loss: 11.899744, test_loss: 214.86314\n",
      "Epoch: 3, train_loss: 11.896772, test_loss: 214.86263\n",
      "Epoch: 4, train_loss: 11.893879, test_loss: 214.86209\n",
      "Epoch: 5, train_loss: 11.891066, test_loss: 214.86159\n",
      "Epoch: 6, train_loss: 11.88833, test_loss: 214.86111\n",
      "Epoch: 7, train_loss: 11.885667, test_loss: 214.86067\n",
      "Epoch: 8, train_loss: 11.883078, test_loss: 214.86029\n",
      "Epoch: 9, train_loss: 11.88056, test_loss: 214.8599\n",
      "Epoch: 10, train_loss: 11.878111, test_loss: 214.85953\n",
      "Epoch: 11, train_loss: 11.8757305, test_loss: 214.8592\n",
      "Epoch: 12, train_loss: 11.873411, test_loss: 214.85887\n",
      "Epoch: 13, train_loss: 11.871157, test_loss: 214.8586\n",
      "Epoch: 14, train_loss: 11.868963, test_loss: 214.8583\n",
      "Epoch: 15, train_loss: 11.866831, test_loss: 214.85805\n",
      "Epoch: 16, train_loss: 11.864754, test_loss: 214.85785\n",
      "Epoch: 17, train_loss: 11.862736, test_loss: 214.85764\n",
      "Epoch: 18, train_loss: 11.860772, test_loss: 214.85744\n",
      "Epoch: 19, train_loss: 11.858858, test_loss: 214.85728\n",
      "Epoch: 20, train_loss: 11.857003, test_loss: 214.85715\n",
      "Epoch: 21, train_loss: 11.855191, test_loss: 214.85701\n",
      "Epoch: 22, train_loss: 11.853433, test_loss: 214.8569\n",
      "Epoch: 23, train_loss: 11.851717, test_loss: 214.85681\n",
      "Epoch: 24, train_loss: 11.850051, test_loss: 214.85675\n",
      "Epoch: 25, train_loss: 11.848429, test_loss: 214.85669\n",
      "Epoch: 26, train_loss: 11.846853, test_loss: 214.85666\n",
      "Epoch: 27, train_loss: 11.845317, test_loss: 214.85664\n",
      "Epoch: 28, train_loss: 11.843824, test_loss: 214.85664\n",
      "Epoch: 29, train_loss: 11.842371, test_loss: 214.85666\n",
      "Epoch: 30, train_loss: 11.840951, test_loss: 214.85669\n",
      "Epoch: 31, train_loss: 11.839576, test_loss: 214.85672\n",
      "Epoch: 32, train_loss: 11.838236, test_loss: 214.85677\n",
      "Epoch: 33, train_loss: 11.836932, test_loss: 214.85686\n",
      "Epoch: 34, train_loss: 11.835664, test_loss: 214.85695\n",
      "Epoch: 35, train_loss: 11.834427, test_loss: 214.85703\n",
      "Epoch: 36, train_loss: 11.833225, test_loss: 214.85716\n",
      "Epoch: 37, train_loss: 11.832056, test_loss: 214.85728\n",
      "Epoch: 38, train_loss: 11.830915, test_loss: 214.85742\n",
      "Epoch: 39, train_loss: 11.829808, test_loss: 214.85757\n",
      "Epoch: 40, train_loss: 11.828731, test_loss: 214.85773\n",
      "Epoch: 41, train_loss: 11.82768, test_loss: 214.8579\n",
      "Epoch: 42, train_loss: 11.826658, test_loss: 214.8581\n",
      "Epoch: 43, train_loss: 11.825666, test_loss: 214.85829\n",
      "Epoch: 44, train_loss: 11.824694, test_loss: 214.8585\n",
      "Epoch: 45, train_loss: 11.823754, test_loss: 214.85872\n",
      "Epoch: 46, train_loss: 11.822836, test_loss: 214.85896\n",
      "Epoch: 47, train_loss: 11.821942, test_loss: 214.8592\n",
      "Epoch: 48, train_loss: 11.821074, test_loss: 214.85944\n",
      "Epoch: 49, train_loss: 11.820225, test_loss: 214.85973\n",
      "Epoch: 50, train_loss: 11.819403, test_loss: 214.85999\n",
      "Epoch: 51, train_loss: 11.818602, test_loss: 214.86024\n",
      "Epoch: 52, train_loss: 11.8178215, test_loss: 214.86055\n",
      "Epoch: 53, train_loss: 11.817063, test_loss: 214.86086\n",
      "Epoch: 54, train_loss: 11.816323, test_loss: 214.86116\n",
      "Epoch: 55, train_loss: 11.815603, test_loss: 214.86147\n",
      "Epoch: 56, train_loss: 11.814901, test_loss: 214.86179\n",
      "Epoch: 57, train_loss: 11.81422, test_loss: 214.8621\n",
      "Epoch: 58, train_loss: 11.813558, test_loss: 214.86246\n",
      "Epoch: 59, train_loss: 11.81291, test_loss: 214.86281\n",
      "Epoch: 60, train_loss: 11.812283, test_loss: 214.86314\n",
      "Epoch: 61, train_loss: 11.8116665, test_loss: 214.86353\n",
      "Epoch: 62, train_loss: 11.811068, test_loss: 214.86389\n",
      "Epoch: 63, train_loss: 11.810488, test_loss: 214.86427\n",
      "Epoch: 64, train_loss: 11.809923, test_loss: 214.86465\n",
      "Epoch: 65, train_loss: 11.809374, test_loss: 214.86507\n",
      "Epoch: 66, train_loss: 11.808839, test_loss: 214.86545\n",
      "Epoch: 67, train_loss: 11.808317, test_loss: 214.86584\n",
      "Epoch: 68, train_loss: 11.807807, test_loss: 214.86626\n",
      "Epoch: 69, train_loss: 11.807314, test_loss: 214.86667\n",
      "Epoch: 70, train_loss: 11.806831, test_loss: 214.8671\n",
      "Epoch: 71, train_loss: 11.806364, test_loss: 214.86754\n",
      "Epoch: 72, train_loss: 11.805907, test_loss: 214.86797\n",
      "Epoch: 73, train_loss: 11.805462, test_loss: 214.86841\n",
      "Epoch: 74, train_loss: 11.805032, test_loss: 214.86884\n",
      "Epoch: 75, train_loss: 11.80461, test_loss: 214.86931\n",
      "Epoch: 76, train_loss: 11.804199, test_loss: 214.86975\n",
      "Epoch: 77, train_loss: 11.803799, test_loss: 214.87022\n",
      "Epoch: 78, train_loss: 11.803411, test_loss: 214.8707\n",
      "Epoch: 79, train_loss: 11.803031, test_loss: 214.87115\n",
      "Epoch: 80, train_loss: 11.802661, test_loss: 214.87164\n",
      "Epoch: 81, train_loss: 11.802305, test_loss: 214.87212\n",
      "Epoch: 82, train_loss: 11.801954, test_loss: 214.87262\n",
      "Epoch: 83, train_loss: 11.801615, test_loss: 214.8731\n",
      "Epoch: 84, train_loss: 11.801285, test_loss: 214.8736\n",
      "Epoch: 85, train_loss: 11.800961, test_loss: 214.87408\n",
      "Epoch: 86, train_loss: 11.800649, test_loss: 214.87459\n",
      "Epoch: 87, train_loss: 11.800342, test_loss: 214.87509\n",
      "Epoch: 88, train_loss: 11.800043, test_loss: 214.87561\n",
      "Epoch: 89, train_loss: 11.799757, test_loss: 214.87613\n",
      "Epoch: 90, train_loss: 11.799475, test_loss: 214.87665\n",
      "Epoch: 91, train_loss: 11.7992, test_loss: 214.87717\n",
      "Epoch: 92, train_loss: 11.798932, test_loss: 214.87769\n",
      "Epoch: 93, train_loss: 11.798666, test_loss: 214.87823\n",
      "Epoch: 94, train_loss: 11.798416, test_loss: 214.87877\n",
      "Epoch: 95, train_loss: 11.798167, test_loss: 214.8793\n",
      "Epoch: 96, train_loss: 11.797927, test_loss: 214.87987\n",
      "Epoch: 97, train_loss: 11.797696, test_loss: 214.88039\n",
      "Epoch: 98, train_loss: 11.797467, test_loss: 214.88095\n",
      "Epoch: 99, train_loss: 11.797246, test_loss: 214.88147\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "lr = 0.003\n",
    "wd = 3\n",
    "first = True\n",
    "trainer = Trainer(model.collect_params(), 'sgd', {'learning_rate': lr, 'wd': wd})\n",
    "model.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "for epoch in range(num_epochs):\n",
    "    for features, labels in get_iterator(train_ds):\n",
    "        batch_size = features.shape[0]\n",
    "        if first:\n",
    "            print(f'Batch size={batch_size}')\n",
    "            first = False\n",
    "        with autograd.record():\n",
    "            yhat = model(features)\n",
    "            loss = loss_fn(yhat, labels)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "    print(f'Epoch: {epoch}, train_loss: {get_ds_loss(train_ds, model)}, test_loss: {get_ds_loss(test_ds, model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(get_iterator(train_ds))[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
