{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx, autograd\n",
    "npx.set_np()\n",
    "from d2l import mxnet as d2l\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 200\n",
    "n = 200\n",
    "X = np.random.normal(size=(n, p))\n",
    "w = np.random.normal(size=(p, 1))\n",
    "b = np.random.normal()\n",
    "y = np.dot(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(X)\n",
    "train_size_fraction = 0.1\n",
    "shuffled_indices = np.random.shuffle(np.array(range(dataset_size)))\n",
    "train_indices = shuffled_indices[:int(train_size_fraction*dataset_size)]\n",
    "test_indices = shuffled_indices[int(train_size_fraction*dataset_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = (X[train_indices, :], y[train_indices]), (X[test_indices, :], y[test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iterator(ds, batch_size=100):\n",
    "    ds_size = len(ds[0])\n",
    "    for start_idx in range(0, ds_size, batch_size):\n",
    "        indices = range(start_idx, min(start_idx + batch_size, ds_size))\n",
    "        yield ds[0][indices], ds[1][indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = get_iterator(train_ds), get_iterator(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X, w, b):\n",
    "    yhat = np.dot(X, w) + b\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(yhat, y, w, lmbd=0.1):\n",
    "    squared_loss = (yhat - y)**2\n",
    "    l2_loss = lmbd*(w**2)\n",
    "    loss = squared_loss + l2_loss.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, b, lr, batch_size):\n",
    "    w[:] = w - lr*w.grad/batch_size\n",
    "    b[:] = b - lr*b.grad/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_loss(ds, w, b):\n",
    "    mse = 0\n",
    "    num_samples = 0\n",
    "    for X, y in get_iterator(ds):\n",
    "        yhat = net(X, w, b)\n",
    "        mse += ((yhat - y)**2).sum()\n",
    "        num_samples += y.shape[0]\n",
    "    return mse/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "what = np.random.normal(size=(p, 1))\n",
    "bhat = np.zeros(1)\n",
    "what.attach_grad()\n",
    "bhat.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train_loss: 366.4363, test_loss: 435.57132\n",
      "Epoch: 1, train_loss: 312.25183, test_loss: 424.059\n",
      "Epoch: 2, train_loss: 267.39398, test_loss: 413.49216\n",
      "Epoch: 3, train_loss: 230.12769, test_loss: 403.74088\n",
      "Epoch: 4, train_loss: 199.05779, test_loss: 394.6988\n",
      "Epoch: 5, train_loss: 173.05994, test_loss: 386.27805\n",
      "Epoch: 6, train_loss: 151.2259, test_loss: 378.40594\n",
      "Epoch: 7, train_loss: 132.82059, test_loss: 371.022\n",
      "Epoch: 8, train_loss: 117.24751, test_loss: 364.07544\n",
      "Epoch: 9, train_loss: 104.02136, test_loss: 357.52335\n",
      "Epoch: 10, train_loss: 92.74638, test_loss: 351.32953\n",
      "Epoch: 11, train_loss: 83.09896, test_loss: 345.46286\n",
      "Epoch: 12, train_loss: 74.81373, test_loss: 339.8966\n",
      "Epoch: 13, train_loss: 67.67244, test_loss: 334.60764\n",
      "Epoch: 14, train_loss: 61.495197, test_loss: 329.57568\n",
      "Epoch: 15, train_loss: 56.13313, test_loss: 324.78293\n",
      "Epoch: 16, train_loss: 51.462715, test_loss: 320.21362\n",
      "Epoch: 17, train_loss: 47.3812, test_loss: 315.85373\n",
      "Epoch: 18, train_loss: 43.80281, test_loss: 311.69058\n",
      "Epoch: 19, train_loss: 40.655678, test_loss: 307.71283\n",
      "Epoch: 20, train_loss: 37.879433, test_loss: 303.91013\n",
      "Epoch: 21, train_loss: 35.42325, test_loss: 300.27292\n",
      "Epoch: 22, train_loss: 33.244102, test_loss: 296.79263\n",
      "Epoch: 23, train_loss: 31.305502, test_loss: 293.46124\n",
      "Epoch: 24, train_loss: 29.576412, test_loss: 290.2713\n",
      "Epoch: 25, train_loss: 28.030376, test_loss: 287.21597\n",
      "Epoch: 26, train_loss: 26.644688, test_loss: 284.2888\n",
      "Epoch: 27, train_loss: 25.399858, test_loss: 281.48383\n",
      "Epoch: 28, train_loss: 24.27914, test_loss: 278.79535\n",
      "Epoch: 29, train_loss: 23.268024, test_loss: 276.21814\n",
      "Epoch: 30, train_loss: 22.353964, test_loss: 273.7472\n",
      "Epoch: 31, train_loss: 21.526035, test_loss: 271.37775\n",
      "Epoch: 32, train_loss: 20.77475, test_loss: 269.1054\n",
      "Epoch: 33, train_loss: 20.091797, test_loss: 266.9259\n",
      "Epoch: 34, train_loss: 19.469921, test_loss: 264.83533\n",
      "Epoch: 35, train_loss: 18.902712, test_loss: 262.8298\n",
      "Epoch: 36, train_loss: 18.384579, test_loss: 260.90576\n",
      "Epoch: 37, train_loss: 17.910564, test_loss: 259.05975\n",
      "Epoch: 38, train_loss: 17.476284, test_loss: 257.2885\n",
      "Epoch: 39, train_loss: 17.077858, test_loss: 255.58888\n",
      "Epoch: 40, train_loss: 16.711842, test_loss: 253.958\n",
      "Epoch: 41, train_loss: 16.375162, test_loss: 252.3929\n",
      "Epoch: 42, train_loss: 16.065113, test_loss: 250.89098\n",
      "Epoch: 43, train_loss: 15.779211, test_loss: 249.44962\n",
      "Epoch: 44, train_loss: 15.515318, test_loss: 248.06631\n",
      "Epoch: 45, train_loss: 15.271441, test_loss: 246.73875\n",
      "Epoch: 46, train_loss: 15.045848, test_loss: 245.46468\n",
      "Epoch: 47, train_loss: 14.836962, test_loss: 244.24188\n",
      "Epoch: 48, train_loss: 14.643332, test_loss: 243.06836\n",
      "Epoch: 49, train_loss: 14.463689, test_loss: 241.94211\n",
      "Epoch: 50, train_loss: 14.296888, test_loss: 240.86116\n",
      "Epoch: 51, train_loss: 14.141833, test_loss: 239.82382\n",
      "Epoch: 52, train_loss: 13.997618, test_loss: 238.8283\n",
      "Epoch: 53, train_loss: 13.863362, test_loss: 237.8729\n",
      "Epoch: 54, train_loss: 13.738304, test_loss: 236.95604\n",
      "Epoch: 55, train_loss: 13.621684, test_loss: 236.07622\n",
      "Epoch: 56, train_loss: 13.51288, test_loss: 235.2319\n",
      "Epoch: 57, train_loss: 13.411311, test_loss: 234.42174\n",
      "Epoch: 58, train_loss: 13.316397, test_loss: 233.6444\n",
      "Epoch: 59, train_loss: 13.227692, test_loss: 232.89853\n",
      "Epoch: 60, train_loss: 13.144704, test_loss: 232.18289\n",
      "Epoch: 61, train_loss: 13.067026, test_loss: 231.49635\n",
      "Epoch: 62, train_loss: 12.9942875, test_loss: 230.83768\n",
      "Epoch: 63, train_loss: 12.926125, test_loss: 230.20581\n",
      "Epoch: 64, train_loss: 12.862238, test_loss: 229.59975\n",
      "Epoch: 65, train_loss: 12.80229, test_loss: 229.01839\n",
      "Epoch: 66, train_loss: 12.746049, test_loss: 228.46077\n",
      "Epoch: 67, train_loss: 12.693241, test_loss: 227.92603\n",
      "Epoch: 68, train_loss: 12.643652, test_loss: 227.4132\n",
      "Epoch: 69, train_loss: 12.597044, test_loss: 226.92145\n",
      "Epoch: 70, train_loss: 12.553233, test_loss: 226.44994\n",
      "Epoch: 71, train_loss: 12.512044, test_loss: 225.99788\n",
      "Epoch: 72, train_loss: 12.473283, test_loss: 225.5645\n",
      "Epoch: 73, train_loss: 12.436815, test_loss: 225.1491\n",
      "Epoch: 74, train_loss: 12.402473, test_loss: 224.75092\n",
      "Epoch: 75, train_loss: 12.370131, test_loss: 224.3693\n",
      "Epoch: 76, train_loss: 12.3396635, test_loss: 224.0036\n",
      "Epoch: 77, train_loss: 12.310977, test_loss: 223.65321\n",
      "Epoch: 78, train_loss: 12.283919, test_loss: 223.31752\n",
      "Epoch: 79, train_loss: 12.2584095, test_loss: 222.99593\n",
      "Epoch: 80, train_loss: 12.23435, test_loss: 222.6879\n",
      "Epoch: 81, train_loss: 12.211642, test_loss: 222.39288\n",
      "Epoch: 82, train_loss: 12.190233, test_loss: 222.1104\n",
      "Epoch: 83, train_loss: 12.17001, test_loss: 221.8399\n",
      "Epoch: 84, train_loss: 12.150927, test_loss: 221.58096\n",
      "Epoch: 85, train_loss: 12.132918, test_loss: 221.33311\n",
      "Epoch: 86, train_loss: 12.115902, test_loss: 221.09592\n",
      "Epoch: 87, train_loss: 12.099805, test_loss: 220.86897\n",
      "Epoch: 88, train_loss: 12.084622, test_loss: 220.65182\n",
      "Epoch: 89, train_loss: 12.070272, test_loss: 220.44409\n",
      "Epoch: 90, train_loss: 12.056693, test_loss: 220.24544\n",
      "Epoch: 91, train_loss: 12.043874, test_loss: 220.05551\n",
      "Epoch: 92, train_loss: 12.031748, test_loss: 219.87392\n",
      "Epoch: 93, train_loss: 12.020273, test_loss: 219.70033\n",
      "Epoch: 94, train_loss: 12.00942, test_loss: 219.53445\n",
      "Epoch: 95, train_loss: 11.999171, test_loss: 219.37598\n",
      "Epoch: 96, train_loss: 11.989461, test_loss: 219.2246\n",
      "Epoch: 97, train_loss: 11.980282, test_loss: 219.08003\n",
      "Epoch: 98, train_loss: 11.971564, test_loss: 218.94202\n",
      "Epoch: 99, train_loss: 11.963349, test_loss: 218.81029\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "lr = 0.003\n",
    "for epoch in range(num_epochs):\n",
    "    for features, labels in get_iterator(train_ds):\n",
    "        batch_size = features.shape[0]\n",
    "        with autograd.record():\n",
    "            yhat = net(features, what, bhat)\n",
    "            loss = loss_fn(yhat, labels, what, lmbd=3)\n",
    "        loss.backward()\n",
    "        sgd(what, bhat, lr, batch_size)\n",
    "    print(f'Epoch: {epoch}, train_loss: {get_ds_loss(train_ds, what, bhat)}, test_loss: {get_ds_loss(test_ds, what, bhat)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
