{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx, autograd\n",
    "from d2l import mxnet as d2l\n",
    "npx.set_np()\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iterator(ds, batch_size=100):\n",
    "    features, labels = ds\n",
    "    num_observations = len(labels)\n",
    "    for start_idx in range(0, num_observations, batch_size):\n",
    "        end_idx = min(num_observations, (start_idx + batch_size))\n",
    "        yield features[start_idx:end_idx], labels[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "train_dl, test_dl = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_hidden1, num_hidden2, num_outputs = 28*28, 256, 256, 10\n",
    "W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hidden1))\n",
    "W2 = np.random.normal(scale=0.01, size=(num_hidden1, num_hidden2))\n",
    "W3 = np.random.normal(scale=0.01, size=(num_hidden2, num_outputs))\n",
    "b1 = np.zeros(shape=(1, num_hidden1))\n",
    "b2 = np.zeros(shape=(1, num_hidden2))\n",
    "b3 = np.zeros(shape=(1, num_outputs))\n",
    "params = [(W1, b1), (W2, b2), (W3, b3)]\n",
    "for W, b in params:\n",
    "    W.attach_grad()\n",
    "    b.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(activations, dropout_prob=0.1):\n",
    "    if dropout_prob == 1:\n",
    "        return np.zeros_like(activations)\n",
    "    elif dropout_prob == 0:\n",
    "        return activations\n",
    "    mask = (np.random.uniform(size=activations.shape) > dropout_prob).astype(int)\n",
    "    return (activations*mask)/(1 - dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, params, dropout_prob=[0]*(len(params) - 1), is_train=False):\n",
    "    final_layer_idx = len(params) - 1\n",
    "    activations = X.reshape(len(X), -1)\n",
    "    for idx, (W, b) in enumerate(params):\n",
    "        activations = np.dot(activations, W)\n",
    "        if idx == final_layer_idx:\n",
    "            break\n",
    "        activations = npx.relu(activations)\n",
    "        if is_train:\n",
    "            activations = dropout_layer(activations, dropout_prob[idx])\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers for Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(output_activations):\n",
    "    exponentiated_activations = np.exp(output_activations)\n",
    "    partition_function = exponentiated_activations.sum(axis=1, keepdims=True)\n",
    "    return exponentiated_activations/partition_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_softmax(probs):\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_from_logits(yhat, y):\n",
    "    num_samples = y.shape[0]\n",
    "    yhat = softmax(yhat)\n",
    "    return -np.log(yhat[range(num_samples), y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.302482 , 2.3025331])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = softmax(act)\n",
    "loss_fn(yhat, np.array([0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, batch_size, lr):\n",
    "    for W, b in params:\n",
    "        W[:] = W - W.grad*lr/batch_size\n",
    "        b[:] = b - b.grad*lr/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.4073313\n",
      "Epoch: 1, Loss: 0.39693454\n",
      "Epoch: 2, Loss: 0.32485893\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "dropout_prob = [0.1, 0.3]\n",
    "lr = 0.1\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in iter(train_dl):\n",
    "        batch_size = X.shape[0]\n",
    "        with autograd.record():\n",
    "            yhat = model(X, params, dropout_prob, True)\n",
    "            loss = softmax_from_logits(yhat, y)\n",
    "        loss.backward()\n",
    "        sgd(params, batch_size, lr)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(dl, model):\n",
    "    num_samples = 0\n",
    "    num_correct = 0\n",
    "    for X, y in iter(dl):\n",
    "        logit_yhat = model(X, params)\n",
    "        yhat = softmax(logit_yhat)\n",
    "        preds = get_labels_from_softmax(yhat)\n",
    "        num_correct += (preds.astype('int32') == y).sum()\n",
    "        num_samples += X.shape[0]\n",
    "    return num_correct/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.8595)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(test_dl, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
